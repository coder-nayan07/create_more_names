{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1098288,"sourceType":"datasetVersion","datasetId":613951}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:18.511624Z","iopub.execute_input":"2025-07-14T11:22:18.511909Z","iopub.status.idle":"2025-07-14T11:22:24.103056Z","shell.execute_reply.started":"2025-07-14T11:22:18.511875Z","shell.execute_reply":"2025-07-14T11:22:24.102143Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"with open('/kaggle/input/indian-names-boys-girls/Names.txt', 'r', encoding='utf-8') as f:\n    words = f.readlines()\n\n# Strip whitespace/newlines\nwords = [name.strip() for name in words]\n\n# Show first few names\nprint(len(words))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:42.958596Z","iopub.execute_input":"2025-07-14T11:22:42.959349Z","iopub.status.idle":"2025-07-14T11:22:43.012217Z","shell.execute_reply.started":"2025-07-14T11:22:42.959317Z","shell.execute_reply":"2025-07-14T11:22:43.010967Z"}},"outputs":[{"name":"stdout","text":"55691\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"names = [n.lower() for n in words]\n\ncleaned_names = []\nfor n in names:\n    vn = ''\n    for ch in n:\n        if ch in [' ', '-', '.']:\n            break\n        vn += ch\n    cleaned_names.append(vn)\n\nwords = cleaned_names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:45.192431Z","iopub.execute_input":"2025-07-14T11:22:45.192823Z","iopub.status.idle":"2025-07-14T11:22:45.344645Z","shell.execute_reply.started":"2025-07-14T11:22:45.192795Z","shell.execute_reply":"2025-07-14T11:22:45.343791Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:47.653984Z","iopub.execute_input":"2025-07-14T11:22:47.654411Z","iopub.status.idle":"2025-07-14T11:22:47.667943Z","shell.execute_reply.started":"2025-07-14T11:22:47.654371Z","shell.execute_reply":"2025-07-14T11:22:47.666848Z"}},"outputs":[{"name":"stdout","text":"{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:49.994104Z","iopub.execute_input":"2025-07-14T11:22:49.994517Z","iopub.status.idle":"2025-07-14T11:22:51.410157Z","shell.execute_reply.started":"2025-07-14T11:22:49.994485Z","shell.execute_reply":"2025-07-14T11:22:51.408771Z"}},"outputs":[{"name":"stdout","text":"torch.Size([400268, 3]) torch.Size([400268])\ntorch.Size([49979, 3]) torch.Size([49979])\ntorch.Size([50069, 3]) torch.Size([50069])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:51.990613Z","iopub.execute_input":"2025-07-14T11:22:51.990927Z","iopub.status.idle":"2025-07-14T11:22:51.996328Z","shell.execute_reply.started":"2025-07-14T11:22:51.990903Z","shell.execute_reply":"2025-07-14T11:22:51.995451Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"n_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:53.734710Z","iopub.execute_input":"2025-07-14T11:22:53.736315Z","iopub.status.idle":"2025-07-14T11:22:53.787068Z","shell.execute_reply.started":"2025-07-14T11:22:53.736255Z","shell.execute_reply":"2025-07-14T11:22:53.786135Z"}},"outputs":[{"name":"stdout","text":"4137\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"batch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:56.100726Z","iopub.execute_input":"2025-07-14T11:22:56.101060Z","iopub.status.idle":"2025-07-14T11:22:56.124867Z","shell.execute_reply.started":"2025-07-14T11:22:56.101033Z","shell.execute_reply":"2025-07-14T11:22:56.123820Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"emb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\nloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:22:58.369184Z","iopub.execute_input":"2025-07-14T11:22:58.369523Z","iopub.status.idle":"2025-07-14T11:22:58.501910Z","shell.execute_reply.started":"2025-07-14T11:22:58.369498Z","shell.execute_reply":"2025-07-14T11:22:58.501134Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor(3.5383, grad_fn=<NegBackward0>)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"bnraw.shape, bnvar_inv.shape , bndiff.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:23:03.150700Z","iopub.execute_input":"2025-07-14T11:23:03.151036Z","iopub.status.idle":"2025-07-14T11:23:03.158341Z","shell.execute_reply.started":"2025-07-14T11:23:03.151014Z","shell.execute_reply":"2025-07-14T11:23:03.156981Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 64]), torch.Size([1, 64]), torch.Size([32, 64]))"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# HERE CALCULATING BACKPROP MANUALLY \n\n# -------------CODE STARTS HERE ----------------\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(n), Yb] = -1.0/n\ndprobs = (1.0 / probs) * dlogprobs\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\ndcounts = counts_sum_inv * dprobs\ndcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\ndcounts += torch.ones_like(counts) * dcounts_sum\ndnorm_logits = counts * dcounts\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\ndh = dlogits @ W2.T\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\ndhpreact = (1.0 - h**2) * dh\ndbngain = (bnraw * dhpreact).sum(0, keepdim=True)\ndbnraw = bngain * dhpreact\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbnvar_inv = (dbnraw * bndiff).sum(0, keepdim = True)\ndbndiff = ( bnvar_inv * dbnraw )\ndbnvar = -0.5 * (bnvar + 1e-5)**(-1.5) * dbnvar_inv\ndbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar \ndbndiff += 2 * bndiff * dbndiff2\ndbnmeani = -1 * (dbndiff * torch.ones_like(bnmeani)).sum(0, keepdim=True)\ndhprebn = dbndiff.clone()\ndhprebn += (1.0/n) * (torch.ones_like(hprebn)) * dbnmeani\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\ndemb = dembcat.view(emb.shape)\ndC = torch.zeros_like(C)\nfor j in range (Xb.shape[0]):\n    for k in range (Xb.shape[1]):\n        ix = Xb[j, k]\n        dC[ix] += demb[j, k]\n\n# ----------------------------------------------\n\ncmp('logprobs', dlogprobs, logprobs)\ncmp('probs', dprobs, probs)\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\ncmp('counts_sum', dcounts_sum, counts_sum)\ncmp('counts', dcounts, counts)\ncmp('norm_logits', dnorm_logits, norm_logits)\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\ncmp('logits', dlogits, logits)\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\ncmp('bnmeani', dbnmeani, bnmeani)\ncmp('hprebn', dhprebn, hprebn)\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\ncmp('emb', demb, emb)\ncmp('C', dC, C)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:23:06.585729Z","iopub.execute_input":"2025-07-14T11:23:06.586916Z","iopub.status.idle":"2025-07-14T11:23:06.627926Z","shell.execute_reply.started":"2025-07-14T11:23:06.586888Z","shell.execute_reply":"2025-07-14T11:23:06.626670Z"}},"outputs":[{"name":"stdout","text":"logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nhpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbngain          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\nbnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\nbndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbnmeani         | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\nhprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nembcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\nW1              | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\nb1              | exact: False | approximate: True  | maxdiff: 8.032657206058502e-09\nemb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\nC               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}